{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # to do summary operations  on data\n",
    "import numpy as np # to do mathematical calculations on data\n",
    "import os # to interact with local system directories\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import seaborn as sns # for better plotting\n",
    "import sys #  To Interact with System folders/libraries\n",
    "from scipy.stats import chi2_contingency  # FOr  Chi square Test\n",
    "os.path.dirname(sys.executable) #  To Interact with System folders/libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "\n",
    "\n",
    "def  create_frequncy_tables_plot(data,col_categorical):\n",
    "    \"\"\" This  function will take the data frame and categorical columns as input and\n",
    "    will give the counts and  proportions of each label  in univariate categorical variables\n",
    "    \"\"\"\n",
    "    # couunts  using  count_values and  proportions using cross table \n",
    "    cross_tab=pd.crosstab(col_categorical ,columns=\"count\")\n",
    "\n",
    "    cross_tab=pd.crosstab(col_categorical ,columns=\"count_percentage\").apply(lambda r: r/len(data), axis=1)\n",
    "    return churn_customers,cross_tab\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def change_data_type(data,col_names,convert_type):\n",
    "    \n",
    "    \"\"\"\" This  function will take the data frame and  columns  and conversion type as input  \n",
    "      will give the  converted  columns from one datatype to  another datatype\n",
    "    \"\"\"\n",
    "    for col in col_names:\n",
    "        print(col ,  \"before  convert\" , data[col].dtype)\n",
    "        data[col] = data[col].astype(convert_type)\n",
    "        print(col,\"after convert\",data[col].dtype)\n",
    "    return data\n",
    "    \n",
    "    \n",
    "\n",
    "def plot_box(data, cols, col_x):\n",
    "    \"\"\"\" This  function will display the  box plot, to show  relationship between  numeric \n",
    "    variables(Cols) and and target categorical variable (Col_x)\n",
    "      \"\"\"\n",
    "    for col in cols:\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.boxplot(col_x, col, data=data)\n",
    "        plt.xlabel(col_x) # Set text for the x axis\n",
    "        plt.ylabel(col)# Set text for y axis\n",
    "        plt.show()\n",
    "\n",
    "def plot_violin(data, cols, col_x):\n",
    "    \"\"\"\" This  function will display the  violin plots to show  relationship between  numeric \n",
    "    variables(Cols) and and target categorical variable (Col_x)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.violinplot(col_x, col, data=data)\n",
    "        plt.xlabel(col_x) # Set text for the x axis\n",
    "        plt.ylabel(col)# Set text for y axis\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "def group_plot(data , cat_columns,target_col):\n",
    "    \"\"\" This  function will show the relationship between two categorical variables in grouped bar chart\"\"\"\n",
    "    for col in cat_columns:\n",
    "        print(col ,\"and\",target_col,\" target Variable\")\n",
    "        carat_table = pd.crosstab(index=data[col],columns=data[target_col])\n",
    "        #print(carat_table)\n",
    "        carat_table.plot(kind=\"bar\", figsize=(10,10),stacked=False)\n",
    "        \n",
    "\n",
    "\n",
    "def encoding_categorical(data , cat_columns):\n",
    "    \n",
    "    \"\"\"  This function  will take  take  dataframe and  cat_columns  as input  and  turn those categorical values into encoding form\n",
    "    0's and 1'  if columns is not category it will convert this into category and encode the  values\n",
    "    \"\"\"\n",
    "    \n",
    "    for col in cat_columns:\n",
    "        data[col]=data[col].astype(\"category\")\n",
    "        data[col]=data[col].cat.codes\n",
    "    return data\n",
    "\n",
    "#missing  values and Outlier  Analysis\n",
    "\n",
    "def treat_outlier(data,numeric_columns):\n",
    "    \"\"\"  This  function will take the input as data frame and numeric values  and return output  dataframe \n",
    "           after  treating the outliers\"\"\"\n",
    "    for i in numeric_columns:\n",
    "        print(i)\n",
    "        q75, q25 = np.percentile(data.loc[:,i], [75 ,25])\n",
    "        iqr = q75 - q25\n",
    "        mini = q25 - (iqr*1.5)\n",
    "        maxi = q75 + (iqr*1.5)\n",
    "        print(mini)\n",
    "        print(maxi)\n",
    "        data_outlier = data.drop(data[data.loc[:,i] < mini].index)\n",
    "        data_outlier = data.drop(data[data.loc[:,i] > maxi].index)\n",
    "    return  data_outlier\n",
    "\n",
    "# function to  show chi- square test results between two categorical variables\n",
    "def chi_square_test(data,cat_columns,target_column):\n",
    "    #loop for chi square values\n",
    "    for i in cat_columns:\n",
    "        print(i)\n",
    "        chi2, p, dof, ex = chi2_contingency(pd.crosstab(data[target_column], data[i]))\n",
    "        print(p)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fun_numeric_relation(data):\n",
    "    \"\"\"\" This function will give  output of  plot of relationship between numeric variables in  data frame  \"\"\"\n",
    "    f, ax = plt.subplots(figsize=(10, 8))\n",
    "    corr = data.corr()\n",
    "    sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax)\n",
    "\n",
    "\n",
    "# This function will convert numeric data into standardization form\n",
    "def standardform_convert(data ,numeric_columns):\n",
    "    \"\"\"  This  functin will take  input as data frame and numerical columns  and convert those numerical data into  standardization\n",
    "    form  and  gives  output and converted data frame\"\"\"\n",
    "    for i in numeric_columns:\n",
    "        print(i)\n",
    "        data[i] = (data[i] - data[i].mean())/data[i].std()\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def  decision_tree_model(x_train,y_train,_x_test,y_test):\n",
    "    \"\"\"  This funcion  will take   Trains and Test data as  input  train the model using  Decision tree  and  also give the\n",
    "     accuracy   of the model   , precision ,recall and F1 score\"\"\"\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    max_depth = 10\n",
    "    min_samples_split =5\n",
    "    my_tree = DecisionTreeClassifier(max_depth =max_depth , min_samples_split =min_samples_split, random_state = 1)\n",
    "    my_tree = my_tree.fit(x_train, y_train)\n",
    "\n",
    "    #Print the score of the new decison tree\n",
    "    print(my_tree.score(x_train, y_train))\n",
    "\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    y_true, y_pred = y_test, my_tree.predict(_x_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "def random_forest_model(x_train,y_train,x_test,y_test):\n",
    "    \"\"\"  This funcion  will take   Trains and Test data as  input  train the model using  Random Forest  and  also give the\n",
    "     accuracy   of the model   , precision ,recall and F1 score  and it will also plot variable importance plot using Random Forest\"\"\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    #Create a Gaussian Classifier\n",
    "    random_f1=RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "    #Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "    random_f1=random_f1.fit(x_train,y_train)\n",
    "\n",
    "    #Print the score of the new decison tree\n",
    "    print(random_f1.score(x_train, y_train))\n",
    "\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    y_true, y_pred = y_test, random_f1.predict(x_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    RF_variable_importance(x_train,random_f1)\n",
    "    \n",
    "\n",
    "def RF_variable_importance(x_train,rf_model):\n",
    "    \"\"\" this  function will take Random forest , X train data as input and  plot the graph of variable importance  as output\n",
    "    \"\"\"\n",
    "    feature_imp = pd.Series(rf_model.feature_importances_,index=x_train.columns).sort_values(ascending=False)\n",
    "    # Creating a bar plot\n",
    "    sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "    # Add labels to your graph\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title(\"Visualizing Important Features\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    train  churm data  from  .csv file\n",
    "\n",
    "# getting and setting  current working directories\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir(\"D:/Edwisor assignments/churn and unchurn/\")\n",
    "os.getcwd()\n",
    "\n",
    "#get the list of files in the  directy\n",
    "\n",
    "print(os.listdir(os.getcwd()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load Train and  Test data\n",
    "\n",
    "df_churn_train = pd.read_csv(\"Train_data.csv\")\n",
    "df_churn_test = pd.read_csv(\"Test_data.csv\")\n",
    "\n",
    "# Renaming column names for train and test data\n",
    "\n",
    "\n",
    "\n",
    "df_churn_train.columns =[\"state\",\"account_length\",\"area_code\",\"phone_number\",\"international_plan\" ,\"voice_mail_plan\",\"number_vmail_messages\",\n",
    "                        \"total_day_minutes\",\"total_day_calls\",\"total_day_charge\",\"total_eve_minutes\",\"total_eve_calls\",\n",
    "                        \"total_eve_charge\",\"total_night_minutes\",\"total_night_calls\",\"total_night_charge\",\n",
    "                        \"total_intl_minutes\",\"total_intl_calls\",\"total_intl_charge\",\"number_customer_service_calls\" ,\"Churn\"]\n",
    "\n",
    "df_churn_test.columns =[\"state\",\"account_length\",\"area_code\",\"phone_number\",\"international_plan\" ,\"voice_mail_plan\",\"number_vmail_messages\",\n",
    "                       \"total_day_minutes\",\"total_day_calls\",\"total_day_charge\",\"total_eve_minutes\",\"total_eve_calls\",\n",
    "                       \"total_eve_charge\",\"total_night_minutes\",\"total_night_calls\",\"total_night_charge\",\n",
    "                       \"total_intl_minutes\",\"total_intl_calls\",\"total_intl_charge\",\"number_customer_service_calls\" ,\"Churn\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop  Phone number column from data frame \n",
    "\n",
    "df_churn_train=df_churn_train.drop([\"phone_number\"],axis=1)\n",
    "df_churn_test=df_churn_test.drop([\"phone_number\"],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding  data \n",
    "df_churn_train.head()\n",
    "\n",
    "# summary of the  data \n",
    "df_churn_train.info()\n",
    "\n",
    "# this data set contains 3333 rows and 20 columns  out of this 20 columns  five columns are  categorical and remaining  \n",
    "#columns are  Numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyse the mean of the  Numeric columns \n",
    "\n",
    "df_churn_train.describe()\n",
    "\n",
    "# It is showing  that Mean of the  total_day__minutes,total_eve__minutes,total_night__minutes and total_day__calls\n",
    "#total_night__calls and total_eve__calls are almost looking the same  and we have to  check how is the  co-releation between\n",
    "#  between variables  to the other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all categorical columns\n",
    "cat_colnames=[\"state\",\"area_code\",\"international_plan\",\"voice_mail_plan\",\"Churn\"]\n",
    "# Independent categorical columns\n",
    "cat_ind_cname =[\"state\",\"area_code\",\"international_plan\",\"voice_mail_plan\"]\n",
    "target_column = [\"Churn\"]\n",
    "numeric_columns = df_churn_train.select_dtypes(exclude=['object','category']).columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change  categorical data types  for both Train and test Data\n",
    "\n",
    "\n",
    "# giving  above categorical date into below function\n",
    "\n",
    "df_churn_train=change_data_type(df_churn_train,cat_colnames,'category')\n",
    "df_churn_test=change_data_type(df_churn_test,cat_colnames,'category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate  Analysis\n",
    "\n",
    "pd.DataFrame.hist(df_churn_train.loc[:,numeric_columns], figsize = [13,13]);\n",
    "\n",
    "# in the  below  Histogram graph it is  showing that almost all the variables are  normally distributes  except\n",
    "#number_vmail_message,number_customer_service_calls and and total_initoial_calls\n",
    "# if you see the ranges  between  the variables  number_customer_service_calls  having  less range (0.7.5) and highest range\n",
    "# is having for total night minutes  nearly (0,400)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Analysis   of  Target variable \n",
    "# check the  proportion of labels  in the target variable\n",
    "target_value_proportion=(df_churn_train[\"Churn\"].value_counts()/len(df_churn_train)*100)\n",
    "print(target_value_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate  Analysis between  target variable and numerical variable\n",
    "\n",
    "plot_box(df_churn_train,numeric_columns,'Churn')\n",
    "\n",
    "#showing  that  “Total_day_charge” , “Total_intl_charge” and “Number_customer_service_charge”  FOr medians ,IQR and Ranges of  Boxplot is different  for “Unchur” and “Churn”  \n",
    "#so these features are  clearly showing  are important to prediction.\n",
    "\n",
    "#For other features  Boxplot  Median , IQeeriR, Ranges are  looking  almost same. Here it is stating  Feature Engineering is important to find the relationship \n",
    "#between  the variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bivariate  relationship between target variable and categorical variables\n",
    "\n",
    "group_plot(df_churn_train,cat_ind_cname,'Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################   missing  values and Outlier  Analysis ###########################\n",
    "\n",
    "#As we  found Using  info() function on data frame that there is not missing values in data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treat the aoutliers  and plot the  graph\n",
    "df_outlier=treat_outlier(df_churn_train,numeric_columns)\n",
    "\n",
    "\n",
    "pd.DataFrame.hist(df_outlier.loc[:,numeric_columns], figsize = [13,13]);\n",
    "\n",
    "#  We are losing almost 10% of data  after treating aoutliers\n",
    "#  after removing  most of the data losing for \"number_of_customers_calls\" and and still right skewness is present for\n",
    "# \"number_vmail_message\" variable and \"total_intl_calls\" variable\n",
    "# those deleted information might be the  important information especially for \"number_of_customers_calls\" so, here going to develop\n",
    "# the model without treating outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Feature  Engineering #################################\n",
    "#Selection of numeric features  by using  correlation between  numeric variables\n",
    "numeric_data=df_churn_train.loc[:,numeric_columns]\n",
    "\n",
    "fun_numeric_relation(numeric_data)\n",
    "\n",
    "# This plot is showing clearly that relation ship between \"total_day_charge- total_day_minutes\" , \"total_night_charge- total_night_minutes\"\n",
    "# \"total_eve_charge- total_eve_minutes\" and \"total_intl_charge- total_intl_minutes\" are very high so out of this any one variable \n",
    "# is require   to build the model\n",
    "\n",
    "df_churn_train = df_churn_train.drop([\"total_day_minutes\",\"total_eve_minutes\",\"total_night_minutes\",\"total_intl_minutes\"],axis=1)\n",
    "df_churn_test = df_churn_test.drop([\"total_day_minutes\",\"total_eve_minutes\",\"total_night_minutes\",\"total_intl_minutes\"],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  find the relation ship between  categorical variable with respect to target variable using  CHi square Test\n",
    "\n",
    "\n",
    "chi_square_test(df_churn_train,cat_ind_cname,'Churn')\n",
    "\n",
    " #p - values fo columns\n",
    "#  state =0.002296\n",
    "# area_code = 0.9151\n",
    "#international.plan=2.2e-16\n",
    "#voice.mail.plan = 5.151e-09\n",
    "#It is  showing  clearly that  relation ship  between \"area_code\" and \"Churn\" is very low  so better to drop this  column\n",
    "\n",
    "\n",
    "df_churn_train = df_churn_train.drop([\"area_code\"],axis=1)\n",
    "df_churn_test=df_churn_test.drop([\"area_code\"],axis=1)\n",
    "df_churn_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  one more step in  one variable  account length which is  seems like categorical , might be account length are small are \n",
    "# old accounts and Churn rate may be more\n",
    "# will turn account numbers into ranges and make it as categorical columns\n",
    "\n",
    "df_churn_train[\"account_cat\"]= pd.cut(df_churn_train[\"account_length\"],bins=[0,50,100,150,200,250],labels=[0,1,2,3,4])\n",
    "df_churn_test[\"account_cat\"]=pd.cut(df_churn_test[\"account_length\"],bins=[0,50,100,150,200,250],labels=[0,1,2,3,4])\n",
    "account_cat=[\"account_cat\"]\n",
    "\n",
    "# now plot group plot  and see the relationship\n",
    "group_plot(df_churn_train,account_cat,\"Churn\")\n",
    "\n",
    "# Now drop the original account length variable\n",
    "\n",
    "df_churn_train = df_churn_train.drop([\"account_length\"],axis=1)\n",
    "df_churn_test=df_churn_test.drop([\"account_length\"],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################  Scaling Data ##########################################################\n",
    "# As we see that almost all the  numeric variables are in  normalal distribution except  two variables\n",
    "# since our data is also contains  few Outliers  we are better to go standardization for scaling\n",
    "\n",
    "numeric_columns_1=df_churn_train.select_dtypes(exclude=[\"category\",\"object\"]).columns\n",
    "\n",
    "df_churn_train=standardform_convert(df_churn_train,numeric_columns_1)\n",
    "df_churn_test=standardform_convert(df_churn_test,numeric_columns_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################  encoding  categorical Variables\n",
    "\n",
    "cat_colnames_1=['state', 'international_plan', 'voice_mail_plan', 'Churn']\n",
    "\n",
    "df_churn_train=encoding_categorical(df_churn_train,cat_colnames_1)\n",
    "df_churn_test=encoding_categorical(df_churn_test,cat_colnames_1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data  to pass through the ML algorithm\n",
    "x_train = df_churn_train.loc[:, df_churn_train.columns != 'Churn']\n",
    "y_train = df_churn_train.loc[:, df_churn_train.columns == 'Churn']\n",
    "x_test =df_churn_test.loc[:, df_churn_test.columns != 'Churn']\n",
    "y_test =df_churn_test.loc[:, df_churn_test.columns == 'Churn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################  Building  Decision Tree ##################################################\n",
    "\n",
    "#Build Decision tree  Model and Evaluate the model\n",
    "decision_tree_model(x_train,y_train,x_test,y_test)\n",
    "\n",
    "# Accuracy of the  Model is  97% \n",
    "# Precision is  0.85 and Recall is  0.68 and f1 score is 0.94\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Build  Randam  Forest Model  ####################################\n",
    "random_forest_model(x_train,y_train,x_test,y_test) \n",
    "\n",
    "# Performance of this model is  good  when compare  to decision  Tree\n",
    "\n",
    "# Here Precision = 0.98 \n",
    "#  recall is  0.70 \n",
    " #and  F1 score is 0.95\n",
    "\n",
    "    #  as per below  feature  importance plot it is clearly showing that features like account_cat,voice_mail_plan  are very\n",
    "    # less contribution   to the  model so, those features are not so importance we will drop those features from train data and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################  Feature selection  based on  Random Forest \n",
    "x_train_1 = x_train.drop([\"account_cat\",\"voice_mail_plan\"],axis=1)\n",
    "x_test_1 = x_test.drop([\"account_cat\",\"voice_mail_plan\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################   perform Random Forest Model  and check the  Accuracy ###############\n",
    "random_forest_model(x_train_1,y_train,x_test_1,y_test) \n",
    "\n",
    "# Performance of this model is  good  when compare  to decision  Tree\n",
    "\n",
    "# Here Precision = 0.97 \n",
    "#  recall is  0.72 \n",
    " #and  F1 score is 0.95\n",
    "    \n",
    "#  #Recall and F1 score  increased but slight decrease in  precision  so  random forest is the best model for this  dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
